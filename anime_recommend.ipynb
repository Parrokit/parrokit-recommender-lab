{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('./data/animelist-dataset/users-score-2023.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(['Username','Anime Title'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u_encoded, u_class = pd.factorize(df[\"user_id\"], sort=True)   # user_idx\n",
    "# i_encoded, i_class = pd.factorize(df[\"anime_id\"], sort=True)  # item_idx\n",
    "\n",
    "# print(\"--------\\n데이터를 유저 인코딩\")\n",
    "# print(u_encoded[:10])\n",
    "# print(\"---------\\n유저 원본 클래스(집합)\")\n",
    "# print(u_class[:10])\n",
    "# print(\"---------\\n데이터를 애니 인코딩\")\n",
    "# print(i_encoded[:10])\n",
    "# print(\"---------\\n애니 원본 클래스(집합)\")\n",
    "# print(i_class[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'{df.memory_usage(deep=True).sum() / 1024**2 :.2f}MB') # 단위: MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ratings = (\n",
    "    pd.read_csv(\"data/animelist-dataset/users-score-2023.csv\",\n",
    "                usecols=['user_id','anime_id','rating'])\n",
    "            .dropna()\n",
    "            .query(\"rating > 0\")\n",
    ")\n",
    "\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings['user_id'].value_counts().head(80000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가가 가장 많은 사람\n",
    "top_users = ratings['user_id'].value_counts().head(500).index\n",
    "print(top_users)\n",
    "print(top_users.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = ratings[ratings[\"user_id\"].isin(top_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids, users = pd.factorize(filtered['user_id'])\n",
    "item_ids, items = pd.factorize(filtered['anime_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = filtered.assign(user_idx=user_ids, item_idx=item_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered[['user_idx','item_idx','rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users, n_items = len(users), len(items)\n",
    "print(n_users)\n",
    "print(n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, tmp_df = train_test_split(\n",
    "    filtered[['user_idx','item_idx','rating']],\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=filtered['user_idx']\n",
    ")\n",
    "valid_x, test_x = train_test_split(\n",
    "    tmp_df,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=tmp_df['user_idx']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RatingsDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        super().__init__()\n",
    "        self.users = torch.tensor(df['user_idx'].values, dtype=torch.long)\n",
    "        self.items = torch.tensor(df['item_idx'].values, dtype=torch.long)\n",
    "        self.ratings = torch.tensor(df['rating'].values, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.ratings[idx]\n",
    "\n",
    "train_loader = DataLoader(RatingsDataset(train_x), batch_size=4096, shuffle=True)\n",
    "valid_loader = DataLoader(RatingsDataset(valid_x), batch_size=8192)\n",
    "test_loader = DataLoader(RatingsDataset(test_x), batch_size=8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_loader.dataset.users.shape)\n",
    "print(train_loader.dataset.users)\n",
    "print(train_loader.dataset.items)\n",
    "print(train_loader.dataset.ratings)\n",
    "\n",
    "print(valid_loader.dataset.users.shape)\n",
    "print(valid_loader.dataset.users)\n",
    "print(valid_loader.dataset.items)\n",
    "print(valid_loader.dataset.ratings)\n",
    "\n",
    "print(test_loader.dataset.users.shape)\n",
    "print(test_loader.dataset.users)\n",
    "print(test_loader.dataset.items)\n",
    "print(test_loader.dataset.ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization(torch.nn.Module):\n",
    "    def __init__(self, num_users, num_items, factors=32):\n",
    "        super().__init__()\n",
    "        self.user_factors = torch.nn.Embedding(num_users,factors) # (80000, 32)\n",
    "        self.item_factors = torch.nn.Embedding(num_items,factors) # (16471, 32)\n",
    "        torch.nn.init.normal_(self.user_factors.weight, std=0.05)\n",
    "        torch.nn.init.normal_(self.item_factors.weight, std=0.05)\n",
    "    \n",
    "    def forward(self,user_idx, item_idx):\n",
    "        u = self.user_factors(user_idx) # (batch_size, 32)\n",
    "        v = self.item_factors(item_idx) # (batch_size, 32)\n",
    "        return (u*v).sum(dim=1) # 원소별 곱한 후 sigma{32개} -> (batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "model = MatrixFactorization(n_users, n_items, factors=64).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "train_rmse_list = []\n",
    "valid_rmse_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for users_batch, items_batch, ratings_batch in train_loader:\n",
    "        users_batch = users_batch.to(device)\n",
    "        items_batch = items_batch.to(device)\n",
    "        ratings_batch = ratings_batch.to(device).float()\n",
    "\n",
    "        preds = model(users_batch, items_batch)\n",
    "        loss = criterion(preds,ratings_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * len(ratings_batch) # 배치가 다르게 들어가면 평균도 달라지는 거 막는 용\n",
    "\n",
    "    train_rmse = (total_loss / len(train_x)) ** 0.5\n",
    "    train_rmse_list.append(train_rmse)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_valid = 0.0\n",
    "        for users_batch, items_batch, ratings_batch in valid_loader:\n",
    "            users_batch = users_batch.to(device)\n",
    "            items_batch = items_batch.to(device)\n",
    "            ratings_batch = ratings_batch.to(device).float()\n",
    "\n",
    "            preds = model(users_batch, items_batch)\n",
    "            loss = criterion(preds, ratings_batch)\n",
    "\n",
    "            total_valid += loss.item() * len(ratings_batch)\n",
    "        \n",
    "        valid_rmse = (total_valid / len(valid_x)) ** 0.5\n",
    "        valid_rmse_list.append(valid_rmse)\n",
    "    \n",
    "    print(f\"[Epoch: {epoch+1:03d}] train RMSE {train_rmse:.3f} | valid RMSE {valid_rmse:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"mf_weight.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx는 long, 모델과 같은 디바이스(mps)여야 함\n",
    "idx = torch.tensor([1, 44, 12], device=device, dtype=torch.long)\n",
    "\n",
    "# (3, 32) 임베딩\n",
    "item_vecs = model.item_factors(idx)\n",
    "\n",
    "# 1) 단순 평균 (32,)\n",
    "user_vec = item_vecs.mean(dim=0)\n",
    "\n",
    "print(user_vec.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scores = (model.item_factors.weight @ user_vec)# 예: scores[:5] -> array([7.2 , 6.8 , 6.4 , ...])\n",
    "print(scores)\n",
    "\n",
    "top_items = scores.argsort()[:-1][:20]  # 예: top_items -> array([ 105,  320,  250, ...])\n",
    "print(top_items)\n",
    "recommended_anime_ids = [items[int(idx)] for idx in top_items]  # 예: recommended_anime_ids -> [5114, 9253, 32281, ...]\n",
    "print(recommended_anime_ids)  # 예시 출력: [5114, 9253, 11061, 30276, 28977, 21, 11061, 199, 6547, 22535]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# seaborn 스타일 설정\n",
    "# ───────────────────────────────────────────────\n",
    "sns.set_theme(\n",
    "    font=\"AppleGothic\",\n",
    "    style=\"darkgrid\",\n",
    "    rc={\n",
    "    \"axes.unicode_minus\": False, # 한글/음수 표시 깨짐 방지\n",
    "    # \"axes.facecolor\": \"#2b2b2b\",  \n",
    "    # \"figure.facecolor\": \"#2b2b2b\",\n",
    "    # \"savefig.facecolor\": \"#2b2b2b\",\n",
    "    # \"grid.color\": \"#4f4f4f\",\n",
    "    # \"axes.edgecolor\": \"#DDDDDD\",\n",
    "    # \"axes.labelcolor\": \"#DDDDDD\",\n",
    "    # \"text.color\": \"#DDDDDD\",\n",
    "    # \"xtick.color\": \"#DDDDDD\",\n",
    "    # \"ytick.color\": \"#DDDDDD\"\n",
    "    }\n",
    ")  \n",
    "\n",
    "# 데이터프레임으로 정리 (seaborn은 long-form 구조가 보기 좋음)\n",
    "epochs = range(1, len(train_rmse_list) + 1)\n",
    "df = pd.DataFrame({\n",
    "    \"Epoch\": list(epochs) * 2,\n",
    "    \"RMSE\": train_rmse_list + valid_rmse_list,\n",
    "    \"Type\": [\"Train\"] * len(train_rmse_list) + [\"Valid\"] * len(valid_rmse_list)\n",
    "})\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# 시각화\n",
    "# ───────────────────────────────────────────────\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.lineplot(\n",
    "    data=df, x=\"Epoch\", y=\"RMSE\", hue=\"Type\", style=\"Type\",\n",
    "    markers=True, dashes=False,\n",
    "    palette=[\"#A3C4F3\", \"#F7C6C7\"]\n",
    ")\n",
    "plt.title(\"Matrix Factorization 학습 곡선 (RMSE)\", fontsize=10)\n",
    "plt.xlabel(\"Epoch\", fontsize=10)\n",
    "plt.ylabel(\"RMSE\", fontsize=10)\n",
    "plt.legend(title=\"데이터 구분\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # 출력 없음: 최종 평가 모드\n",
    "with torch.no_grad():  # 출력 없음: 그래디언트 비활성화\n",
    "    total_test = 0.0  # 예: 초기값 0.0\n",
    "    for users_batch, items_batch, ratings_batch in test_loader:\n",
    "        preds = model(users_batch.to(device), items_batch.to(device))  # 예: preds[:4] -> tensor([7.48, 6.80, ...])\n",
    "        loss = criterion(preds, ratings_batch.to(device))  # 예: loss.item() -> 1.07\n",
    "        total_test += loss.item() * len(ratings_batch)  # 예: total_test -> 2150.0 (누적)\n",
    "    test_rmse = (total_test / len(test_x)) ** 0.5  # 예: test_rmse -> 1.04\n",
    "\n",
    "print(f\"Test RMSE {test_rmse:.3f}\")  # 예: \"Test RMSE 1.042\"\n",
    "\n",
    "user_example = 0  # 예: 첫 번째 인코딩된 사용자\n",
    "with torch.no_grad():  # 출력 없음\n",
    "    user_vec = model.user_factors.weight[user_example]  # 예: user_vec[:5] -> tensor([0.078, -0.012, 0.044, ...])\n",
    "    scores = (model.item_factors.weight @ user_vec).cpu().numpy()  # 예: scores[:5] -> array([7.2 , 6.8 , 6.4 , ...])\n",
    "top_items = scores.argsort()[::-1][:20]  # 예: top_items -> array([ 105,  320,  250, ...])\n",
    "recommended_anime_ids = [items[idx] for idx in top_items]  # 예: recommended_anime_ids -> [5114, 9253, 32281, ...]\n",
    "recommended_anime_ids  # 예시 출력: [5114, 9253, 11061, 30276, 28977, 21, 11061, 199, 6547, 22535]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime = pd.read_csv(\"data/animelist-dataset/anime-dataset-2023.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_anime = anime[anime[\"anime_id\"].isin(recommended_anime_ids)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "a = pd.read_csv(\"data/animelist-dataset/users-score-2023.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sentence-transformers\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ---------- 0) 노이즈에 강한 정규화 ----------\n",
    "# - 한글/히라가나/가타카나/한자/영문/숫자만 남김\n",
    "# - 괄호/부제/기호/이상문자 제거 후 공백 정리\n",
    "NOISE_KEEP = re.compile(r\"[^0-9A-Za-z\\uAC00-\\uD7A3\\u3040-\\u309F\\u30A0-\\u30FF\\u4E00-\\u9FFF\\s]+\")\n",
    "\n",
    "def normalize_title(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"\\(.*?\\)\", \" \", s)              # 괄호 내 부제 제거\n",
    "    s = NOISE_KEEP.sub(\" \", s)                  # 허용 외 문자 제거\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()          # 공백 정리\n",
    "    return s\n",
    "\n",
    "# ---------- 1) 예시 마스터(원제만; 필요시 CSV로 교체) ----------\n",
    "titles = [\n",
    "    # Naruto-verse\n",
    "    {\"anime_id\": 1,  \"title\": \"Naruto\"},\n",
    "    {\"anime_id\": 2,  \"title\": \"Naruto Shippuden\"},\n",
    "    {\"anime_id\": 3,  \"title\": \"Boruto: Naruto Next Generations\"},\n",
    "    # Big shonen\n",
    "    {\"anime_id\": 4,  \"title\": \"One Piece\"},\n",
    "    {\"anime_id\": 5,  \"title\": \"Bleach\"},\n",
    "    {\"anime_id\": 6,  \"title\": \"Bleach: Thousand-Year Blood War\"},\n",
    "    {\"anime_id\": 7,  \"title\": \"Dragon Ball Z\"},\n",
    "    {\"anime_id\": 8,  \"title\": \"JoJo's Bizarre Adventure\"},\n",
    "    {\"anime_id\": 9,  \"title\": \"Slam Dunk\"},\n",
    "    {\"anime_id\": 10, \"title\": \"Detective Conan\"},\n",
    "    # Modern hits\n",
    "    {\"anime_id\": 11, \"title\": \"Attack on Titan\"},\n",
    "    {\"anime_id\": 12, \"title\": \"Demon Slayer: Kimetsu no Yaiba\"},\n",
    "    {\"anime_id\": 13, \"title\": \"Jujutsu Kaisen\"},\n",
    "    {\"anime_id\": 14, \"title\": \"My Hero Academia\"},\n",
    "    {\"anime_id\": 15, \"title\": \"Chainsaw Man\"},\n",
    "    {\"anime_id\": 16, \"title\": \"SPY×FAMILY\"},\n",
    "    {\"anime_id\": 17, \"title\": \"Haikyu!!\"},\n",
    "    {\"anime_id\": 18, \"title\": \"Blue Lock\"},\n",
    "    {\"anime_id\": 19, \"title\": \"Oshi no Ko\"},\n",
    "    {\"anime_id\": 20, \"title\": \"Frieren: Beyond Journey's End\"},\n",
    "    # Classics\n",
    "    {\"anime_id\": 21, \"title\": \"Fullmetal Alchemist: Brotherhood\"},\n",
    "    {\"anime_id\": 22, \"title\": \"Death Note\"},\n",
    "    {\"anime_id\": 23, \"title\": \"Neon Genesis Evangelion\"},\n",
    "    {\"anime_id\": 24, \"title\": \"Steins;Gate\"},\n",
    "    {\"anime_id\": 25, \"title\": \"Made in Abyss\"},\n",
    "    # SAO / Re:Zero / Mushoku\n",
    "    {\"anime_id\": 26, \"title\": \"Sword Art Online\"},\n",
    "    {\"anime_id\": 27, \"title\": \"Re:Zero − Starting Life in Another World\"},\n",
    "    {\"anime_id\": 28, \"title\": \"Mushoku Tensei: Jobless Reincarnation\"},\n",
    "    # Sports / Misc\n",
    "    {\"anime_id\": 29, \"title\": \"Kaguya-sama: Love Is War\"},\n",
    "    {\"anime_id\": 30, \"title\": \"Vinland Saga\"},\n",
    "    {\"anime_id\": 31, \"title\": \"Dr. Stone\"},\n",
    "    {\"anime_id\": 32, \"title\": \"Mob Psycho 100\"},\n",
    "    # Your examples\n",
    "    {\"anime_id\": 33, \"title\": \"Dandadan\"},\n",
    "]\n",
    "\n",
    "titles_df = pd.DataFrame(titles)\n",
    "\n",
    "# ---------- 2) 인덱스 구축 ----------\n",
    "def build_index(df: pd.DataFrame, model_name: str = \"paraphrase-multilingual-mpnet-base-v2\"):\n",
    "    docs = [normalize_title(t) for t in df[\"title\"].tolist()]\n",
    "    id_map = df[\"anime_id\"].tolist()\n",
    "    model = SentenceTransformer(model_name)\n",
    "    emb = model.encode(docs, normalize_embeddings=True).astype(\"float32\")  # (N, d)\n",
    "    return model, emb, docs, id_map\n",
    "\n",
    "model, emb, docs, id_map = build_index(titles_df)\n",
    "\n",
    "# ---------- 3) 단건 검색 ----------\n",
    "def search_title(q: str, k: int = 5, cutoff: float = 0.55) -> List[Tuple[int, float, str]]:\n",
    "    qn = normalize_title(q)\n",
    "    qv = model.encode([qn], normalize_embeddings=True).astype(\"float32\")[0]\n",
    "    sims = emb @ qv  # 코사인 유사도\n",
    "    k = min(k, len(sims))\n",
    "    idx = np.argpartition(-sims, k-1)[:k]\n",
    "    idx = idx[np.argsort(-sims[idx])]\n",
    "    hits = [(id_map[i], float(sims[i]), docs[i]) for i in idx if sims[i] >= cutoff]\n",
    "    return hits  # [(anime_id, score, matched_norm_title)]\n",
    "\n",
    "# ---------- 4) 배치 검색 (여러 질의 한 번에) ----------\n",
    "def batch_search(queries: List[str], k: int = 5, cutoff: float = 0.55):\n",
    "    qn = [normalize_title(q) for q in queries]\n",
    "    qv = model.encode(qn, normalize_embeddings=True).astype(\"float32\")  # (B, d)\n",
    "    sims = qv @ emb.T  # (B, N)\n",
    "    results = []\n",
    "    for r in range(sims.shape[0]):\n",
    "        row = sims[r]\n",
    "        kk = min(k, len(row))\n",
    "        idx = np.argpartition(-row, kk-1)[:kk]\n",
    "        idx = idx[np.argsort(-row[idx])]\n",
    "        hits = [(id_map[i], float(row[i]), docs[i]) for i in idx if row[i] >= cutoff]\n",
    "        results.append(hits)\n",
    "    return results\n",
    "\n",
    "# ---------- 5) 데모 ----------\n",
    "queries = [\n",
    "    \"나루토 질풍전ㅣㄴㅁ;ㅣㅇ;\",   # → Naruto Shippuden\n",
    "    \"단다단1-₩129812ㅑ\",          # → Dandadan\n",
    "    \"귀멸의 칼날\",                 # → Demon Slayer: Kimetsu no Yaiba (ko 표기)\n",
    "    \"진격의거인!!\",               # → Attack on Titan\n",
    "    \"강철의 연금술사 브라더후드\",   # → Fullmetal Alchemist: Brotherhood\n",
    "    \"스파이 패밀리\",               # → SPY×FAMILY\n",
    "    \"재:제로\",                     # → Re:Zero − Starting Life in Another World\n",
    "]\n",
    "for q, hits in zip(queries, batch_search(queries, k=5, cutoff=0.50)):\n",
    "    print(q, \"->\", hits[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "a = pd.read_csv(\"data/animelist-dataset/users-score-2023.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_pairs = set(zip(a[\"anime_id\"], a[\"Anime Title\"]))\n",
    "print(anime_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(t[1] for t in list(anime_pairs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sentence-transformers\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Optional, Iterable\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ---------------- 0) 정규화 ----------------\n",
    "NOISE_KEEP = re.compile(r\"[^0-9A-Za-z\\uAC00-\\uD7A3\\u3040-\\u309F\\u30A0-\\u30FF\\u4E00-\\u9FFF\\s]+\")\n",
    "\n",
    "def normalize_title(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"\\(.*?\\)\", \" \", s)      # 괄호 내 부제 제거\n",
    "    s = NOISE_KEEP.sub(\" \", s)          # 허용 외 문자 제거\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()  # 공백 정리\n",
    "    return s\n",
    "\n",
    "# ---------------- 1) anime_pairs -> 인덱스 ----------------\n",
    "def build_index_from_pairs(anime_pairs: Iterable[Tuple[int, str]],\n",
    "                           model_name: str = \"paraphrase-multilingual-mpnet-base-v2\"):\n",
    "    df = pd.DataFrame(anime_pairs, columns=[\"anime_id\", \"title\"])\n",
    "    docs = [normalize_title(t) for t in df[\"title\"].tolist()]\n",
    "    id_map = df[\"anime_id\"].tolist()\n",
    "    title_map = df[\"title\"].tolist()  # 원제 복원용\n",
    "    model = SentenceTransformer(model_name)\n",
    "    emb = model.encode(docs, normalize_embeddings=True).astype(\"float32\")  # (N, d)\n",
    "    return model, emb, docs, id_map, title_map\n",
    "\n",
    "# ---------------- 2) Top-1 검색 ----------------\n",
    "def search_top1(q: str,\n",
    "                model: SentenceTransformer,\n",
    "                emb: np.ndarray,\n",
    "                docs: List[str],\n",
    "                id_map: List[int],\n",
    "                title_map: List[str],\n",
    "                cutoff: float = 0.55) -> Optional[Tuple[int, str, float, str]]:\n",
    "    \"\"\"\n",
    "    반환: (anime_id, original_title, score, matched_norm_title) or None\n",
    "    \"\"\"\n",
    "    qn = normalize_title(q)\n",
    "    qv = model.encode([qn], normalize_embeddings=True).astype(\"float32\")[0]\n",
    "    sims = emb @ qv  # (N,)\n",
    "    i = int(np.argmax(sims))\n",
    "    score = float(sims[i])\n",
    "    if score < cutoff:\n",
    "        return None\n",
    "    return id_map[i], title_map[i], score, docs[i]\n",
    "\n",
    "def batch_top1(queries: List[str],\n",
    "               model: SentenceTransformer,\n",
    "               emb: np.ndarray,\n",
    "               docs: List[str],\n",
    "               id_map: List[int],\n",
    "               title_map: List[str],\n",
    "               cutoff: float = 0.55):\n",
    "    results = []\n",
    "    for q in queries:\n",
    "        hit = search_top1(q, model, emb, docs, id_map, title_map, cutoff=cutoff)\n",
    "        results.append((q, hit))\n",
    "    return results\n",
    "\n",
    "# ---------------- 3) 예시 사용 ----------------\n",
    "# 예: 사용자가 제공한 anime_pairs (id, title) 집합\n",
    "\n",
    "model, emb, docs, id_map, title_map = build_index_from_pairs(anime_pairs)\n",
    "\n",
    "queries = [\n",
    "    \"나루토 질풍전ㅣㄴㅁ;ㅣㅇ;\",   # → Naruto Shippuden\n",
    "    \"단다단1-₩129812ㅑ\",          # → Dandadan\n",
    "    \"귀멸의 칼날\",                 # → Demon Slayer: Kimetsu no Yaiba\n",
    "    \"진격의거인!!\",               # → Attack on Titan\n",
    "    \"강철의 연금술사 브라더후드\",   # → Fullmetal Alchemist: Brotherhood\n",
    "    \"스파이 패밀리\",               # → SPY×FAMILY\n",
    "    \"재:제로\",                     # → Re:Zero − Starting Life in Another World\n",
    "]\n",
    "\n",
    "results = batch_top1(queries, model, emb, docs, id_map, title_map, cutoff=0.50)\n",
    "\n",
    "for q, hit in results:\n",
    "    if hit is None:\n",
    "        print(f\"{q} -> None (below cutoff)\")\n",
    "    else:\n",
    "        aid, title, score, matched_norm = hit\n",
    "        print(f\"{q} -> anime_id={aid}, title='{title}', score={score:.3f}, matched='{matched_norm}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "animet = pd.read_csv(\"data/animelist-dataset/anime-dataset-2023.csv\",usecols=['Other name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = list(animet['Other name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U transformers sentencepiece torch pykakasi --quiet\n",
    "import re, torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from pykakasi import kakasi\n",
    "\n",
    "MODEL = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "tok = AutoTokenizer.from_pretrained(MODEL, use_fast=False)\n",
    "mt  = AutoModelForSeq2SeqLM.from_pretrained(MODEL)\n",
    "mt.eval()\n",
    "\n",
    "kks = kakasi()\n",
    "\n",
    "# (참고) 일본어 → 한국어로 번역\n",
    "def ja2ko(text: str) -> str:\n",
    "    tok.src_lang = \"ja_XX\"\n",
    "    enc = tok(text, return_tensors=\"pt\")\n",
    "    with torch.inference_mode():\n",
    "        out = mt.generate(\n",
    "            **enc,\n",
    "            forced_bos_token_id=tok.convert_tokens_to_ids(\"ko_KR\"),\n",
    "            num_beams=5,\n",
    "            length_penalty=1.1,\n",
    "            max_new_tokens=64,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "    result = tok.batch_decode(out, skip_special_tokens=True)[0].strip()\n",
    "    # 불필요한 접두어 제거\n",
    "    result = re.sub(r\"^(한국어|번역).*?:\", \"\", result).strip()\n",
    "    return result\n",
    "\n",
    "# 언어 판별 유틸\n",
    "def has_kana(s):  return any('\\u3040' <= ch <= '\\u30ff' for ch in s)\n",
    "def has_kanji(s): return any('\\u4e00' <= ch <= '\\u9fff' for ch in s)\n",
    "def has_hangul(s): return any('\\uac00' <= ch <= '\\ud7a3' for ch in s)\n",
    "\n",
    "# 로마자 → 가나 변환\n",
    "def romaji_to_hira(s: str) -> str:\n",
    "    return \"\".join(part[\"hira\"] for part in kks.convert(s)).strip()\n",
    "\n",
    "def to_korean_title(t: str) -> str:\n",
    "    t = t.strip()\n",
    "    if not t:\n",
    "        return \"\"\n",
    "\n",
    "    # 1️⃣ 이미 한글 → 그대로\n",
    "    if has_hangul(t):\n",
    "        return t\n",
    "\n",
    "    # 2️⃣ 일본어(가나 또는 한자 포함) → 일본어→한국어 번역\n",
    "    if has_kana(t) or has_kanji(t):\n",
    "        return ja2ko(t)\n",
    "\n",
    "    # 3️⃣ 로마자 일본어 → 가나 변환 → 번역\n",
    "    hira = romaji_to_hira(t)\n",
    "    if hira:\n",
    "        return ja2ko(hira)\n",
    "\n",
    "    # 4️⃣ 영어류 → 영어→한국어 번역\n",
    "    tok.src_lang = \"en_XX\"\n",
    "    enc = tok(t, return_tensors=\"pt\")\n",
    "    with torch.inference_mode():\n",
    "        out = mt.generate(\n",
    "            **enc,\n",
    "            forced_bos_token_id=tok.convert_tokens_to_ids(\"ko_KR\"),\n",
    "            num_beams=5, length_penalty=1.1, max_new_tokens=64\n",
    "        )\n",
    "    return tok.batch_decode(out, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "\n",
    "\n",
    "# 사용 예시\n",
    "for s in samples:\n",
    "    print(s, \" → \", to_korean_title(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U transformers sentencepiece torch langdetect --quiet\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from langdetect import detect\n",
    "\n",
    "MODEL = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "\n",
    "# 핵심: use_fast=False 로드 → protobuf 불필요\n",
    "tok = AutoTokenizer.from_pretrained(MODEL, use_fast=False)\n",
    "mt  = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL,\n",
    "    use_safetensors=True,\n",
    "    low_cpu_mem_usage=False,\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "mt.eval()\n",
    "\n",
    "def mbart_to_ko(text: str) -> str:\n",
    "    t = text.strip()\n",
    "    if not t:\n",
    "        return \"\"\n",
    "    # 간단 언어 판별 (짧은 제목 보정)\n",
    "    try:\n",
    "        lang = detect(t)\n",
    "    except:\n",
    "        lang = \"en\"\n",
    "    if any('\\u3040' <= ch <= '\\u30ff' for ch in t):  # 히라/가타카나\n",
    "        lang = \"ja\"\n",
    "    if any('\\uac00' <= ch <= '\\ud7a3' for ch in t):  # 이미 한글\n",
    "        return t\n",
    "\n",
    "    # mBART-50 언어 코드\n",
    "    src = \"en_XX\" if lang.startswith(\"en\") else \"ja_XX\"\n",
    "    tok.src_lang = src\n",
    "\n",
    "    enc = tok(t, return_tensors=\"pt\")\n",
    "    with torch.inference_mode():\n",
    "        out_ids = mt.generate(\n",
    "            **enc,\n",
    "            forced_bos_token_id=tok.convert_tokens_to_ids(\"ko_KR\"),\n",
    "            max_new_tokens=64,\n",
    "        )\n",
    "    return tok.batch_decode(out_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "# 테스트\n",
    "# samples = [\n",
    "#     \"Attack on Titan\",\n",
    "#     \"Demon Slayer: Kimetsu no Yaiba\",\n",
    "#     \"ジョジョの奇妙な冒険\",\n",
    "#     \"SPY×FAMILY\",\n",
    "# ]\n",
    "samples = list(t[1] for t in list(anime_pairs))\n",
    "for s in samples:\n",
    "    print(s, \"→\", mbart_to_ko(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 패키지 (한 번만 설치)\n",
    "# pip install -U pykakasi transformers sentencepiece torch langdetect --quiet\n",
    "\n",
    "import re\n",
    "import torch\n",
    "from langdetect import detect\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from pykakasi import kakasi\n",
    "\n",
    "MODEL = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "\n",
    "# mBART 로드\n",
    "tok = AutoTokenizer.from_pretrained(MODEL, use_fast=False)\n",
    "mt  = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL,\n",
    "    use_safetensors=True,\n",
    "    low_cpu_mem_usage=False,\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "mt.eval()\n",
    "\n",
    "# 로마자 -> 가나 변환기\n",
    "kks = kakasi()\n",
    "\n",
    "# 작품명 공식/관용 한글 타이틀 우선 매핑\n",
    "OVERRIDE = {\n",
    "    # 영어/일본어 원제 : 한국 통용 제목\n",
    "    \"Attack on Titan\": \"진격의 거인\",\n",
    "    \"Demon Slayer: Kimetsu no Yaiba\": \"귀멸의 칼날\",\n",
    "    \"SPY×FAMILY\": \"SPY×FAMILY\",  # 보통 그대로 표기\n",
    "    \"SPY X FAMILY\": \"스파이 패밀리\",\n",
    "    \"SPYXFAMILY\": \"스파이 패밀리\",\n",
    "    \"JoJo's Bizarre Adventure\": \"죠죠의 기묘한 모험\",\n",
    "    \"Great Mazinger\": \"그레이트 마징가\",\n",
    "    \"Cop Craft\": \"코프 크래프트\",\n",
    "    \"Penguin Highway\": \"펭귄 하이웨이\",\n",
    "    # 필요시 계속 추가\n",
    "}\n",
    "\n",
    "# 유니코드 범위 체크\n",
    "def has_kana(s: str) -> bool:\n",
    "    return any('\\u3040' <= ch <= '\\u30ff' for ch in s)  # 히라/가타카나\n",
    "\n",
    "def has_hangul(s: str) -> bool:\n",
    "    return any('\\uac00' <= ch <= '\\ud7a3' for ch in s)\n",
    "\n",
    "def is_romaji_like_japanese(s: str) -> bool:\n",
    "    \"\"\"\n",
    "    일본어 로마자를 대략 감지: 한글/가나/한자 없고,\n",
    "    공통 조사/접사/패턴이 포함되면 True\n",
    "    \"\"\"\n",
    "    if has_hangul(s) or has_kana(s):\n",
    "        return False\n",
    "    # 한자 범위\n",
    "    if any('\\u4e00' <= ch <= '\\u9fff' for ch in s):\n",
    "        return False\n",
    "\n",
    "    low = s.lower()\n",
    "    tokens = re.findall(r\"[a-zA-Z]+\", low)\n",
    "    if not tokens:\n",
    "        return False\n",
    "\n",
    "    # 흔한 로마자 일본어 패턴/조사/접사\n",
    "    jp_markers = [\n",
    "        \" no \", \" wa \", \" ga \", \" wo \", \" to \", \" mo \", \" de \", \" ni \",\n",
    "        \"kara \", \" made\", \" shou\", \" chou\", \" jou\", \" kyo\", \" kyou\",\n",
    "        \" gei\", \" geki\", \" gekijou\", \" gekijō\", \" shoku\", \" pan\", \" shokupan\",\n",
    "        \" kanojo\", \" sekai\", \" utsukushii\", \" minikuku\", \" nagerareta\",\n",
    "        \" wakaokami\", \" shougakusei\", \" kandou\", \" e.\", \" reso\", \" nantoka\",\n",
    "    ]\n",
    "    pad = \" \" + low + \" \"\n",
    "    score = sum(1 for m in jp_markers if m in pad)\n",
    "\n",
    "    # 모음-자음 패턴이 과도하게 반복되면 로마자 일본어로 가정\n",
    "    vowel_ratio = sum(ch in \"aeiou\" for ch in low) / max(1, sum(ch.isalpha() for ch in low))\n",
    "    looks_romaji = vowel_ratio > 0.30 and score >= 1\n",
    "    return looks_romaji\n",
    "\n",
    "def en_like(s: str) -> bool:\n",
    "    # 한글/가나/한자 없고, 알파벳+숫자/기호 위주면 영어류로 간주\n",
    "    if has_hangul(s) or has_kana(s):\n",
    "        return False\n",
    "    if any('\\u4e00' <= ch <= '\\u9fff' for ch in s):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def mbart_translate(text: str, src_lang: str, tgt_lang: str = \"ko_KR\",\n",
    "                    prompt_hint: str = \"\", beams: int = 5) -> str:\n",
    "    tok.src_lang = src_lang\n",
    "    inp = prompt_hint + text if prompt_hint else text\n",
    "    enc = tok(inp, return_tensors=\"pt\")\n",
    "    with torch.inference_mode():\n",
    "        out_ids = mt.generate(\n",
    "            **enc,\n",
    "            forced_bos_token_id=tok.convert_tokens_to_ids(tgt_lang),\n",
    "            num_beams=beams,\n",
    "            length_penalty=1.1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            max_new_tokens=64,\n",
    "            early_stopping=True,\n",
    "            do_sample=False,\n",
    "        )\n",
    "    return tok.batch_decode(out_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "def romaji_to_hira(s: str) -> str:\n",
    "    return \"\".join(part[\"hira\"] for part in kks.convert(s)).strip()\n",
    "\n",
    "def mbart_to_ko(text: str) -> str:\n",
    "    t = text.strip()\n",
    "    if not t:\n",
    "        return \"\"\n",
    "\n",
    "    # 0) override 우선\n",
    "    if t in OVERRIDE:\n",
    "        return OVERRIDE[t]\n",
    "\n",
    "    # 1) 이미 한글이면 그대로\n",
    "    if has_hangul(t):\n",
    "        return t\n",
    "\n",
    "    # 2) 가나/한자 → 일본어로 간주 후 번역\n",
    "    if has_kana(t) or any('\\u4e00' <= ch <= '\\u9fff' for ch in t):\n",
    "        return mbart_translate(t, src_lang=\"ja_XX\")\n",
    "\n",
    "    # 3) 로마자 일본어 감지 → 가나 변환 → ja→ko\n",
    "    if is_romaji_like_japanese(t):\n",
    "        hira = romaji_to_hira(t)\n",
    "        if hira:\n",
    "            out = mbart_translate(hira, src_lang=\"ja_XX\")\n",
    "            # 결과가 입력과 거의 동일(복사)하면 힌트 붙여 재시도\n",
    "            norm_in = re.sub(r\"\\W+\", \"\", t).lower()\n",
    "            norm_out = re.sub(r\"\\W+\", \"\", out).lower()\n",
    "            if norm_in == norm_out:\n",
    "                out = mbart_translate(hira, src_lang=\"ja_XX\", prompt_hint=\"한국어 제목으로 번역: \")\n",
    "            return out\n",
    "\n",
    "    # 4) 그 외(대체로 영어류): en→ko 시도\n",
    "    if en_like(t):\n",
    "        out = mbart_translate(t, src_lang=\"en_XX\", prompt_hint=\"Translate to Korean title: \")\n",
    "        # 복사 방지 재시도\n",
    "        norm_in = re.sub(r\"\\W+\", \"\", t).lower()\n",
    "        norm_out = re.sub(r\"\\W+\", \"\", out).lower()\n",
    "        if norm_in == norm_out:\n",
    "            out = mbart_translate(t, src_lang=\"en_XX\", prompt_hint=\"한국어 제목: \")\n",
    "        # 마지막으로 override 스캔(괄호/공백 변형 대비)\n",
    "        if out == t and t.title() in OVERRIDE:\n",
    "            return OVERRIDE[t.title()]\n",
    "        return out\n",
    "\n",
    "    # 5) 실패 시 원문 반환\n",
    "    return t\n",
    "\n",
    "# -------------------------------\n",
    "# 사용 예시: 질문에 주신 방식 그대로\n",
    "samples = list(t[1] for t in list(anime_pairs))\n",
    "for s in samples:\n",
    "    print(s, \"→\", mbart_to_ko(s))\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "MODEL = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "tok = AutoTokenizer.from_pretrained(MODEL, use_fast=False)\n",
    "mt  = AutoModelForSeq2SeqLM.from_pretrained(MODEL)\n",
    "\n",
    "def mbart_en2ko(title: str) -> str:\n",
    "    if not title.strip():\n",
    "        return \"\"\n",
    "    tok.src_lang = \"en_XX\"  # 소스 고정\n",
    "    enc = tok(\"Translate to Korean: \" + title, return_tensors=\"pt\")  # 프롬프트 힌트로 '복사' 성향 완화\n",
    "    with torch.inference_mode():\n",
    "        out = mt.generate(\n",
    "            **enc,\n",
    "            forced_bos_token_id=tok.convert_tokens_to_ids(\"ko_KR\"),\n",
    "            num_beams=5,                # 탐색 확대\n",
    "            length_penalty=1.1,         # 너무 짧게 끝내지 않게\n",
    "            no_repeat_ngram_size=2,\n",
    "            max_new_tokens=64,\n",
    "            early_stopping=True,\n",
    "            do_sample=False,\n",
    "        )\n",
    "    return tok.batch_decode(out, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "for s in [\n",
    "    \"Attack on Titan\",\n",
    "    \"Demon Slayer: Kimetsu no Yaiba\",\n",
    "    \"SPY×FAMILY\",\n",
    "]:\n",
    "    print(s, \"→\", mbart_en2ko(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "tok = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from langdetect import detect\n",
    "\n",
    "MODEL = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL, use_fast=False)\n",
    "mt  = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL,\n",
    "    use_safetensors=True,\n",
    "    low_cpu_mem_usage=False,\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "mt.eval()\n",
    "print(\"model device:\", next(mt.parameters()).device)  # cpu 여야 정상\n",
    "\n",
    "def mbart_to_ko(text: str) -> str:\n",
    "    t = text.strip()\n",
    "    if not t:\n",
    "        return \"\"\n",
    "    if any('\\uac00' <= ch <= '\\ud7a3' for ch in t):\n",
    "        return t\n",
    "    src_lang = \"ja\" if any('\\u3040' <= ch <= '\\u30ff' for ch in t) else \"en\"\n",
    "    tok.src_lang = \"en_XX\" if src_lang==\"en\" else \"ja_XX\"\n",
    "\n",
    "    enc = tok(t, return_tensors=\"pt\")\n",
    "    with torch.inference_mode():\n",
    "        out_ids = mt.generate(\n",
    "            **enc,\n",
    "            forced_bos_token_id=tok.convert_tokens_to_ids(\"ko_KR\"),\n",
    "            max_new_tokens=64,\n",
    "        )\n",
    "    return tok.batch_decode(out_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "# 테스트\n",
    "samples = [\"Attack on Titan\", \"Demon Slayer: Kimetsu no Yaiba\", \"ジョジョの奇妙な冒険\", \"SPY×FAMILY\"]\n",
    "for s in samples:\n",
    "    print(s, \"→\", mbart_to_ko(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 설치 (한 번만)\n",
    "# %pip install -U transformers datasets peft accelerate sentencepiece torch --quiet\n",
    "\n",
    "# %% 환경 체크: MPS\n",
    "import os, torch, random\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "device = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"[device]\", device)\n",
    "\n",
    "# %% 미니 데이터셋 만들기 (메모리)\n",
    "pairs = [\n",
    "    (\"カウボーイビバップ\", \"카우보이 비밥\"),\n",
    "    (\"カウボーイビバップ 天国の扉\", \"카우보이 비밥 천국의 문\"),\n",
    "    (\"トライガン\", \"트라이건\"),\n",
    "    (\"新世紀エヴァンゲリオン\", \"신세기 에반게리온\"),\n",
    "    (\"ナルト\", \"나루토\"),\n",
    "    (\"ONE PIECE\", \"원피스\"),\n",
    "    (\"テニスの王子様\", \"테니스의 왕자\"),\n",
    "    (\"スクールランブル\", \"스쿨 럼블\"),\n",
    "    (\"頭文字〈イニシャル〉D\", \"이니셜 D\"),\n",
    "    (\"頭文字〈イニシャル〉D FOURTH STAGE\", \"이니셜 D 포스 스테이지\"),\n",
    "    (\"ハングリーハート\", \"헝그리 하트\"),\n",
    "    (\"ハングリーハート Wild Striker\", \"헝그리 하트 와일드 스트라이커\"),\n",
    "    (\"ハチミツとクローバー\", \"허니와 클로버\"),\n",
    "    (\"モンスター\", \"몬스터\"),\n",
    "    (\"冒険王ビィト\", \"모험왕 비트\"),\n",
    "    (\"アイシールド21\", \"아이실드 21\"),\n",
    "    (\"機動戦士ガンダム\", \"기동전사 건담\"),\n",
    "    (\"コードギアス 反逆のルルーシュ\", \"코드 기아스 반역의 를르슈\"),\n",
    "    (\"魔法少女まどか☆マギカ\", \"마법소녀 마도카☆마기카\"),\n",
    "    (\"ジパング\", \"지팡\"),\n",
    "    (\"進撃の巨人\", \"진격의 거인\"),\n",
    "    (\"鬼滅の刃\", \"귀멸의 칼날\"),\n",
    "    (\"SPY×FAMILY\", \"스파이 패밀리\"),\n",
    "    (\"ジョジョの奇妙な冒険\", \"죠죠의 기묘한 모험\"),\n",
    "    (\"銀魂\", \"은혼\"),\n",
    "    (\"鋼の錬金術師\", \"강철의 연금술사\"),\n",
    "    (\"デスノート\", \"데스노트\"),\n",
    "    (\"ソードアート・オンライン\", \"소드 아트 온라인\"),\n",
    "    (\"Re:ゼロから始める異世界生活\", \"Re:제로부터 시작하는 이세계 생활\"),\n",
    "    (\"この素晴らしい世界に祝福を！\", \"이 멋진 세계에 축복을!\"),\n",
    "    (\"ノーゲーム・ノーライフ\", \"노 게임 노 라이프\"),\n",
    "    (\"涼宮ハルヒの憂鬱\", \"스즈미야 하루히의 우울\"),\n",
    "    (\"らき☆すた\", \"러키☆스타\"),\n",
    "    (\"けいおん！\", \"케이온!\"),\n",
    "    (\"シュタインズ・ゲート\", \"슈타인즈 게이트\"),\n",
    "    (\"攻殻機動隊\", \"공각기동대\"),\n",
    "    (\"サイコパス\", \"사이코패스\"),\n",
    "    (\"プラスティック・メモリーズ\", \"플라스틱 메모리즈\"),\n",
    "    (\"ヴァイオレット・エヴァーガーデン\", \"바이올렛 에버가든\"),\n",
    "    (\"四月は君の嘘\", \"4월은 너의 거짓말\"),\n",
    "    (\"化物語\", \"바케모노가타리\"),\n",
    "    (\"とある科学の超電磁砲\", \"어떤 과학의 초전자포\"),\n",
    "    (\"とある魔術の禁書目録\", \"어떤 마법의 금서목록\"),\n",
    "    (\"五等分の花嫁\", \"5등분의 신부\"),\n",
    "]\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(pairs)\n",
    "split = int(len(pairs)*0.8)\n",
    "train_pairs, valid_pairs = pairs[:split], pairs[split:]\n",
    "\n",
    "from datasets import Dataset\n",
    "train_ds = Dataset.from_dict({\"ja\":[j for j,_ in train_pairs], \"ko\":[k for _,k in train_pairs]})\n",
    "valid_ds = Dataset.from_dict({\"ja\":[j for j,_ in valid_pairs], \"ko\":[k for _,k in valid_pairs]})\n",
    "\n",
    "print(\"train/valid:\", len(train_ds), len(valid_ds))\n",
    "\n",
    "# %% 모델/토크나이저 로드 (mBART-50)\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "MODEL = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "tok = AutoTokenizer.from_pretrained(MODEL, use_fast=False)\n",
    "base = AutoModelForSeq2SeqLM.from_pretrained(MODEL)\n",
    "base.to(device)\n",
    "\n",
    "# %% LoRA 장착\n",
    "from peft import LoraConfig, get_peft_model\n",
    "peft_cfg = LoraConfig(\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    "    r=16, lora_alpha=32, lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\",\"fc1\",\"fc2\"],  # mBART 구조에 맞춘 핵심 모듈\n",
    ")\n",
    "model = get_peft_model(base, peft_cfg)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# %% 전처리\n",
    "max_src, max_tgt = 64, 64\n",
    "\n",
    "def preprocess(batch):\n",
    "    # 1) 소스/타깃 언어 코드 설정\n",
    "    tok.src_lang = \"ja_XX\"\n",
    "    tok.tgt_lang = \"ko_KR\"\n",
    "\n",
    "    # 2) 입력과 라벨 토크나이즈\n",
    "    model_inputs = tok(batch[\"ja\"], max_length=max_src, truncation=True)\n",
    "\n",
    "    # ※ v4 권장: as_target_tokenizer() 대신 text_target= 사용\n",
    "    labels = tok(text_target=batch[\"ko\"], max_length=max_tgt, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "train_tok = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\n",
    "valid_tok = valid_ds.map(preprocess, batched=True, remove_columns=valid_ds.column_names)\n",
    "\n",
    "# %% 학습 세팅 (MPS 친화)\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tok, model=model)\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"mbart_ja2ko_title_lora_mps\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=8,\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=64,\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=valid_tok,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tok,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# %% 어댑터만 저장\n",
    "model.save_pretrained(\"mbart_ja2ko_title_lora_mps/adapter\")\n",
    "\n",
    "# %% 간단 추론 함수\n",
    "import re\n",
    "def infer(title: str) -> str:\n",
    "    tok.src_lang = \"ja_XX\"\n",
    "    enc = tok(title, return_tensors=\"pt\").to(device)\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(\n",
    "            **enc,\n",
    "            forced_bos_token_id=tok.convert_tokens_to_ids(\"ko_KR\"),\n",
    "            num_beams=5, length_penalty=1.1, max_new_tokens=64, early_stopping=True\n",
    "        )\n",
    "    text = tok.batch_decode(out, skip_special_tokens=True)[0].strip()\n",
    "    # 혹시 모델이 접두사 붙이면 제거\n",
    "    text = re.sub(r\"^(한국어|번역).*?:\\s*\", \"\", text).strip()\n",
    "    return text\n",
    "\n",
    "tests = [\n",
    "    \"カウボーイビバップ\",\n",
    "    \"カウボーイビバップ 天国の扉\",\n",
    "    \"頭文字〈イニシャル〉D FOURTH STAGE\",\n",
    "    \"ハチミツとクローバー\",\n",
    "    \"鬼滅の刃\",\n",
    "    \"ジョジョの奇妙な冒険\",\n",
    "]\n",
    "for t in samples:\n",
    "    print(t, \"→\", infer(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch, accelerate, transformers, datasets, peft\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"accelerate:\", accelerate.__version__)\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"datasets:\", datasets.__version__)\n",
    "print(\"peft:\", peft.__version__)\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sju-project-rec_anime",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
